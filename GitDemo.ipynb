{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fca7ef28-9b1f-469f-a2ef-19c8b0d8cf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import numpy as np\n",
    "import torch\n",
    "import lightning.pytorch as pl\n",
    "import torchmetrics\n",
    "import torchvision\n",
    "from torchinfo import summary\n",
    "from torchview import draw_graph\n",
    "from IPython.display import display\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import json\n",
    "from torch.nn.functional import pad\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f3444b-a1be-4964-b14c-b70b7e9bd18e",
   "metadata": {},
   "source": [
    "## Load the tokenizer and model of your choice ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58714db-09b9-4dbd-9c17-a62587a2ec3f",
   "metadata": {},
   "source": [
    "### GPT2 Tokenizer ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f16e5528-4f62-425d-8bd2-8e466f1c6bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gpt2 = GPT2Model.from_pretrained('gpt2')\n",
    "tokenizer_gpt2 = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer_gpt2.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2a85a6-2a04-4139-b95b-8fb644d2ab07",
   "metadata": {},
   "source": [
    "### BERT Tokenizer ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2116be8-ce4e-4a96-84c2-27c3a34dd7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_bert = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65560436-86a9-4841-a579-41e9f8288068",
   "metadata": {},
   "source": [
    "### BART ###\n",
    "Uncomment this if you want to see summaries of the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9486c0bf-e8c7-42cb-aa42-6396df513fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BART model and tokenizer\n",
    "#tokenizer_bart = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "#model_bart = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d8b687c-36e2-458e-a817-8bee04e79712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only CPU is available...\n"
     ]
    }
   ],
   "source": [
    "#Check system requirements\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name())\n",
    "    print(torch.cuda.get_device_properties(\"cuda\"))\n",
    "    print(\"Number of devices:\", torch.cuda.device_count())\n",
    "    device = (\"cuda\")\n",
    "else:\n",
    "    print(\"Only CPU is available...\")\n",
    "    device = (\"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed5479b-62ce-4ef9-923c-8a833c262b9c",
   "metadata": {},
   "source": [
    "# Encoder Networks #\n",
    "\n",
    "This network will encode the conversation using BERT/GPT2 as word embeddings to be passed into the classifier networks later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5180ca4-e309-49c8-93c5-d9fff46e058d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class encoderNetwork(Dataset):\n",
    "    def __init__(self, conversations, targets, tokenizer, pretrainedModel):\n",
    "        self.conversations = conversations  # data\n",
    "        self.model = pretrainedModel  # Model choice\n",
    "        self.targets = targets  # Target Labels\n",
    "        self.tokenizer = tokenizer  # Tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.conversations)\n",
    "\n",
    "    # Convert from list of sentence strings to one long string, encode the string\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if self.model == 'BERT':\n",
    "            segmentedText = [f'{sentence} [SEP][CLS]' for sentence in self.conversations[idx]]\n",
    "            text = \" \".join(segmentedText)\n",
    "        else: #GPT2\n",
    "             text = \" \".join(self.conversations[idx])\n",
    "        \n",
    "        encoding = self.tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        input_ids = encoding[\"input_ids\"].squeeze()\n",
    "        attention_mask = encoding[\"attention_mask\"].squeeze()\n",
    "        label = torch.tensor(self.targets[idx])\n",
    "\n",
    "        return input_ids, attention_mask, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d7d576-c160-4975-8e0a-f2c85c3d0ae5",
   "metadata": {},
   "source": [
    "# Classifier Networks #\n",
    "Note: Each classifier network requires the corresponding pretrained model from above to initizlize. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fa494f-1dd4-4002-8c60-65bb5b022065",
   "metadata": {},
   "source": [
    "### GPT2 Classifier ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b12b433d-9197-47a7-bbae-c4f42f3b36c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify the conversation with GPT2 as the base model\n",
    "class GPT2ClassifierNetwork(pl.LightningModule):\n",
    "    def __init__(self, gpt2_model=GPT2Model.from_pretrained('gpt2')):\n",
    "        super(GPT2ClassifierNetwork, self).__init__()\n",
    "\n",
    "        # Freeze GPT-2 weights\n",
    "        #Done for space limitation on GPU\n",
    "        for param in gpt2_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.gpt2 = gpt2_model\n",
    "        self.fc1 = nn.Linear(self.gpt2.config.hidden_size, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 2)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.accuracy = torchmetrics.classification.Accuracy(task='binary')\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.gpt2(input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_states = outputs['last_hidden_state']\n",
    "\n",
    "        # Take the mean of Last hidden states along the sequence dimension\n",
    "        pooled_output = torch.mean(last_hidden_states, dim=1)\n",
    "\n",
    "        # Apply linear layers\n",
    "        x = F.relu(self.fc1(pooled_output))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        logits = self.fc3(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        logits = self(input_ids, attention_mask)\n",
    "        loss = self.loss_fn(logits, labels)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = self.accuracy(preds, labels)\n",
    "\n",
    "        # Log metrics\n",
    "        self.log('train_acc', acc, on_step=False, on_epoch=True)\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        logits = self(input_ids, attention_mask)\n",
    "        loss = self.loss_fn(logits, labels)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = self.accuracy(preds, labels)\n",
    "\n",
    "        # Log metrics\n",
    "        self.log('val_acc', acc, on_step=False, on_epoch=True)\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True)\n",
    "\n",
    "        return {\"val_loss\": loss, \"val_acc\": acc}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa3554d-224b-45db-83ac-f4808f94797b",
   "metadata": {},
   "source": [
    "### BERT Classifier ###\n",
    "\n",
    "This classifier has an extra linear layer, uses the BERT specific pooler output instead of taking the average of the last hidden state like the GPT2 model, and could be loaded onto the GPU without freezing BERTS weights. Otherwise, it is the same as the GPT2 version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6be947a6-949f-47af-9f50-567eba6392a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classify the conversation\n",
    "class BERTClassifierNetwork(pl.LightningModule):\n",
    "    def __init__(self, bert_model=BertModel.from_pretrained('bert-base-uncased')):\n",
    "        super(BERTClassifierNetwork, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.fc1 = nn.Linear(self.bert.config.hidden_size, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 2)  # Binary classification, so output size is 2\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.accuracy = torchmetrics.classification.Accuracy(task='binary')\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs['pooler_output']\n",
    "\n",
    "        # Apply linear layers\n",
    "        x = F.relu(self.fc1(pooled_output))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        logits = self.fc4(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        logits = self(input_ids, attention_mask)\n",
    "        loss = self.loss_fn(logits, labels)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = self.accuracy(preds, labels)\n",
    "\n",
    "        # Log metrics\n",
    "        self.log('train_acc', acc, on_step=False, on_epoch=True)\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        logits = self(input_ids, attention_mask)\n",
    "        loss = self.loss_fn(logits, labels)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = self.accuracy(preds, labels)\n",
    "\n",
    "        # Log metrics\n",
    "        self.log('val_acc', acc, on_step=False, on_epoch=True)\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True)\n",
    "\n",
    "        return {\"val_loss\": loss, \"val_acc\": acc}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51367d1-af57-471d-8399-baaf124be0af",
   "metadata": {},
   "source": [
    "## Helper Functions ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03e8d9fb-34e5-4eec-a285-57c00f0365e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences within each batch got some errors on data size so this fixes that\n",
    "#For Data loaders\n",
    "def custom_collate_fn(batch):\n",
    "    input_ids, attention_masks, labels = zip(*batch)\n",
    "\n",
    "    # Determine the maximum length in the batch\n",
    "    max_len = max(len(ids) for ids in input_ids)\n",
    "\n",
    "    # Pad / truncate sequences to the max length using the padding token (0)\n",
    "    padded_input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    padded_attention_masks = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "\n",
    "    return padded_input_ids, padded_attention_masks, torch.tensor(labels)\n",
    "\n",
    "# Get the E-redial input data from the file\n",
    "def get_input_data(filename):\n",
    "    # Fetch jsonFile from URL\n",
    "    response = requests.get(filename)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        messages = json.loads(response.text)\n",
    "    else:\n",
    "        # Handle unsuccessful request (e.g., print an error message)\n",
    "        print(f\"Failed to fetch data from {filename}\")\n",
    "        return None\n",
    "    #Open jsonFile ONLY if you have the file locally\n",
    "    #with open(filename, 'r', encoding='utf-8') as f:\n",
    "        #messages = json.load(f)\n",
    "\n",
    "    #Loop through message and grab sentences\n",
    "    wholeConv = {}                  #{[helperText],[HelperText]}\n",
    "    forPretrainConv = []\n",
    "    convRoleList = []\n",
    "    idList = []\n",
    "    \n",
    "    #Will manage when a turn has occured\n",
    "    seekerSpoken = False\n",
    "    helperSpoken = False\n",
    "    \n",
    "    for message in messages:\n",
    "        preservedOrderList = []\n",
    "        totalHelperMssg = []\n",
    "        totalSeekerMssg = []\n",
    "        cur_roles = []\n",
    "        workin = message['messages']\n",
    "        id = message['conversationId']\n",
    "        idList.append(id)\n",
    "        for dictionary in workin:\n",
    "            role = dictionary['role']\n",
    "            sentence = dictionary['text']\n",
    "            #Manage conversation turn tracking \n",
    "            if role == 1:\n",
    "                totalHelperMssg.append(sentence)\n",
    "                \n",
    "            else: \n",
    "                totalSeekerMssg.append(sentence)\n",
    "                #Adjust 0 role to 2 for NN processing later\n",
    "                role = 2\n",
    "            cur_roles.append(role)\n",
    "            \n",
    "            preservedOrderList.append(sentence) #Add sentence to this conversation\n",
    "\n",
    "        #add conversation to list of conversations\n",
    "        convRoleList.append(cur_roles)\n",
    "        forPretrainConv.append(preservedOrderList)\n",
    "        #Add info to conv containers\n",
    "        wholeConv[hash(id)] = [totalHelperMssg, totalSeekerMssg]\n",
    "        #joinedConversations = combineConsecutiveSpeakerSentences(forPretrainConv, convRoleList)\n",
    "    #return forPretrainConv, convRoleList\n",
    "    return wholeConv, idList,forPretrainConv, convRoleList #,joinedConversations\n",
    "\n",
    " \n",
    "#Combine consectuive utterances into a single sentence so any seperation\n",
    "#In the conversations are by different speakers\n",
    "def combineConsecutiveSpeakerSentences(input_lists, roles_list):\n",
    "    joined_strings_list = []\n",
    "\n",
    "    #Input list is a whole conversation, role list is a parallel array with the\n",
    "    #role order of that conversations\n",
    "    for input_list, role_list in zip(input_lists, roles_list):\n",
    "        joined_strings = []\n",
    "        current_role = None\n",
    "        current_string = \"\"\n",
    "\n",
    "        #For each sentence and role in the conversation\n",
    "        for text, role in zip(input_list, role_list):\n",
    "            if current_role is None:\n",
    "                # First iteration\n",
    "                current_role = role\n",
    "                current_string = text\n",
    "            elif current_role == role:\n",
    "                # Same role, concatenate the strings\n",
    "                current_string += \" \" + text\n",
    "            else:\n",
    "                # Different role, add combined input to list, \n",
    "                # then start a new string\n",
    "                joined_strings.append(current_string)\n",
    "                current_role = role\n",
    "                current_string = text\n",
    "\n",
    "        # Append the last string\n",
    "        joined_strings.append(current_string)\n",
    "        joined_strings_list.append(joined_strings)\n",
    "\n",
    "    return joined_strings_list\n",
    "\n",
    "# Get target data from the file  # Data stored as convId,target\n",
    "def get_target_data(filename):\n",
    "    targets = []\n",
    "    # Fetch jsonFile from URL\n",
    "    response = requests.get(filename)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        content = response.text\n",
    "    else:\n",
    "        # Handle unsuccessful request (e.g., print an error message)\n",
    "        print(f\"Failed to fetch data from {filename}\")\n",
    "        return None\n",
    "    #with open(filename, 'r') as file:\n",
    "    file = content.split('\\n')\n",
    "    for line in file:\n",
    "        values = line.strip().split(',')\n",
    "        if len(values) ==2: #ignore any empty lines\n",
    "            #Ignore convID, just grab target label, convert to tensor\n",
    "            targets.append(torch.tensor(int(values[1])))\n",
    "\n",
    "    return np.array(targets)\n",
    "\n",
    "\n",
    "# Function to pad conversations to a common length\n",
    "def pad_conversations(conversations):\n",
    "    #Get maxlength conversation, and pad each conversation so theyre the same length\n",
    "    max_length = max(len(conv) for conv in conversations)\n",
    "    padded_conversations = []\n",
    "    for conv in conversations:\n",
    "        padded_conv = conv + [''] * (max_length - len(conv))\n",
    "        padded_conversations.append(padded_conv)\n",
    "    return padded_conversations\n",
    "\n",
    "#Show the bart summary of a conversation\n",
    "def showBartSummary(idList, wholeConv):\n",
    "    #Loop through every conversation in the list\n",
    "    targetScores = {} #hash(ID), score 0/1 good/bad\n",
    "    count = 0\n",
    "    for x in range(2): #id in idList:\n",
    "        id = idList[x]\n",
    "        count+=1\n",
    "        print('Working on id#',count)\n",
    "        #Get current conversation, split into recommender and seeker (left/right)\n",
    "        curConv = wholeConv[hash(id)]\n",
    "        leftString = ' '.join(curConv[0])\n",
    "        rightString = ' '.join(curConv[1])\n",
    "\n",
    "        #Make summary of the conversation\n",
    "        leftSummary = generateSummaryBart(leftString)\n",
    "        rightSummary = generateSummaryBart(rightString)\n",
    "        print(f'*****Conversation #{count}*****\\n')\n",
    "        print('Left Summary:\\n', leftSummary)\n",
    "        print('\\nRight Summary:\\n',rightSummary)\n",
    "        print('\\n\\n')\n",
    "\n",
    "#This function generate summaries using BART. \n",
    "def generateSummaryBart(text):\n",
    "    inputs = tokenizer_bart(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    summary_ids = model_bart.generate(**inputs, max_length=150, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer_bart.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e77e0d1-6a22-48e8-8210-9160d0251bce",
   "metadata": {},
   "source": [
    "# E-redial Data #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d526aa47-b9b6-41b3-ac2d-ad269aa3466a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup input files.\n",
    "data = 'https://f23deeplearning-eredial-classifier-demo.nyc3.digitaloceanspaces.com/test.json'\n",
    "\n",
    "# Setup target files\n",
    "targets = 'https://f23deeplearning-eredial-classifier-demo.nyc3.digitaloceanspaces.com/E-redial-TEST-LABELS.txt'\n",
    "\n",
    "#Get input data from file\n",
    "wholeConv, idList, convData, convRoleList = get_input_data(data)  \n",
    "\n",
    "\n",
    "testInput = combineConsecutiveSpeakerSentences(convData, convRoleList)\n",
    "\n",
    "#Get target labels \n",
    "testLabels = get_target_data(targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4de7f3-c5a3-45ec-939f-d8ee21b4982a",
   "metadata": {},
   "source": [
    "What does the data look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c2cffca-9ee1-4210-b4ef-56544d2a960c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "******Conversation Number:0******\n",
      "Sentence #0 -- Hello\n",
      "Sentence #1 -- Hi! How are you?\n",
      "Sentence #2 -- Good.\n",
      "Sentence #3 -- What are you looking for today?\n",
      "Sentence #4 -- Well, I enjoy romantic comedy, and chic movies! I really like [Steel Magnolias (1989)]. I've seen it a few times.\n",
      "Sentence #5 -- As you enjoy romantic comedy, and chic movies,[Girls Trip (2017)] might be something you're interested in.When four lifelong friends travel to New Orleans for the annual Essence Festival, sisterhoods are rekindled,wild sides are rediscovered,and there's enough dancing drinking brawling and romancing to make the Big Easy blush.You may like it once you watched.\n",
      "Sentence #6 -- Yes, I have heard great things about that one. I want to see it soon.\n",
      "Sentence #7 -- Since you like romantic comedy, and chic movies , then I think these movie well suit your taste[Divine Secrets of the Ya-Ya Sisterhood (2002)]I really liked was that they DIDN'T fall back on that old chestnut of somebody dying to serve as a convenient catalyst for change and the healing process.I can guarantee you that this movie will leave you a good experience.\n",
      "Sentence #8 -- Yes, those are some of my favorites as well. Yes these are really good. I just love Julia Roberts! She's great.\n",
      "Sentence #9 -- If you love romantic comedy and Julia Roberts, you should try [Notting Hill] . It's about the life of a simple bookshop owner being changed when he meets the most famous film star in the world. Also [Runaway Bride (1999)], it's about a reporter assigned to write a story about a woman who has left a string of fiancés at the altar. Try both of them and you'll love them!\n",
      "Sentence #10 -- Yes, her movies are cute! She does a good job in them Thanks for these suggestions!\\\n",
      "Sentence #11 -- Sure! Have a great day!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "******Conversation Number:1******\n",
      "Sentence #0 -- Hi\n",
      "Sentence #1 -- Hello! Can you suggest any kids movies?\n",
      "Sentence #2 -- I suggest [Leap Year (2010)] and [The Dark Knight (2008)]. Since they are both good kid movies. [Leap Year (2010)] is a romantic comedy about a woman who travels to Ireland to propose to her boyfriend on Leap Day, only to find herself on a wild journey with a handsome innkeeper. [The Dark Knight (2008)] is a superhero film that follows the journey of Batman as he battles against the Joker to save Gotham city from chaos and destruction. I highly recommend them!\n",
      "Sentence #3 -- I have seen [The Dark Knight  (2008)] but not [Leap Year  (2010)] got any other random suggestions. I will take whatever\n",
      "Sentence #4 -- Did you like it\n",
      "Sentence #5 -- Yes It was good\n",
      "Sentence #6 -- I would recommend [Interstellar (2014)]. Because it's a good science film and its thought-provoking depiction of humanity's future and our relationship with time and space. The film follows a team of astronauts who travel through a wormhole in search of a new home for humanity as Earth is on the brink of extinction. Hope you will like it!\n",
      "Sentence #7 -- That is a great one! I loved it.\n",
      "Sentence #8 -- Because you like kids movies , so I recommend you to see[Wonder (2017)]Simply put, this is the perfect family film for all ages. Outstanding acting by little Canadian phenom Jacob Tremblay as well as a great supporting cast. I can guarantee you that this movie will leave you a good experience.\n",
      "Sentence #9 -- Oh, that one! I was going to see that. Thanks for the reminder!\n",
      "Sentence #10 -- Anytime Have a good day\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Show 2 conversations \n",
    "for convNum in range(2):\n",
    "    print(f'\\n\\n******Conversation Number:{convNum}******')\n",
    "    for utterance, sentence in enumerate(testInput[convNum]):\n",
    "        print(f'Sentence #{utterance} -- {sentence}')\n",
    "    print('\\n')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1962afea-2372-445a-82b2-05a2f425d19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment Only if you have loaded BART to look at summaries\n",
    "#Show 2 conversation summaries\n",
    "#showBartSummary(idList,wholeConv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24fd377-ae38-4b4d-95b6-78f4704d6606",
   "metadata": {},
   "source": [
    "# Classifier Showcase #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17feeac3-4039-42c4-a537-adac9ae7271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "TEST_BATCH_SIZE = 5\n",
    "NUM_EPOCHS = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76de6ae-3429-4486-92f1-f3add4e03f69",
   "metadata": {},
   "source": [
    "### GPT2 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdd9b1a9-c691-4319-9b1f-5eecc53f5d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Downloading: \"https://f23deeplearning-eredial-classifier-demo.nyc3.digitaloceanspaces.com/gpt2-epoch=59-step=11340-v6.ckpt\" to /home/jovyan/.cache/torch/hub/checkpoints/gpt2-epoch=59-step=11340-v6.ckpt\n",
      "100%|██████████| 481M/481M [00:16<00:00, 30.3MB/s] \n",
      "/opt/conda/lib/python3.11/site-packages/lightning/fabric/loggers/csv_logs.py:195: UserWarning: Experiment logs directory lightning_logs/ClassifierTest/gpt2 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0831cca8aeae423f8d2223dcf477b4f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">  Runningstage.validating  </span>┃<span style=\"font-weight: bold\">                           </span>┃\n",
       "┃<span style=\"font-weight: bold\">          metric           </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          val_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6466666460037231     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.664914071559906     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m Runningstage.validating \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m                           \u001b[0m┃\n",
       "┃\u001b[1m \u001b[0m\u001b[1m         metric          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m         val_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6466666460037231    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.664914071559906    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'val_acc': 0.6466666460037231, 'val_loss': 0.664914071559906}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Embed the data so it can be used by the classifiers\n",
    "gpt2_test_dataset = encoderNetwork(testInput,  testLabels, tokenizer_gpt2, 'GPT2')\n",
    "#Make data loaders\n",
    "gpt2_test_dataloader = DataLoader(gpt2_test_dataset, \n",
    "                                  batch_size=TEST_BATCH_SIZE, \n",
    "                                  shuffle=False, collate_fn=custom_collate_fn, \n",
    "                                  num_workers=8)\n",
    "#Make logger\n",
    "gpt2_logger = pl.loggers.CSVLogger(\"lightning_logs\", name=\"ClassifierTest\", version=\"gpt2\")\n",
    "\n",
    "#Make trainer\n",
    "gpt2_trainer = pl.Trainer(\n",
    "        logger=gpt2_logger,\n",
    "        max_epochs=NUM_EPOCHS,\n",
    "        enable_progress_bar=True,\n",
    "        log_every_n_steps=0,\n",
    "        enable_checkpointing=True,\n",
    "        callbacks=[pl.callbacks.TQDMProgressBar(refresh_rate=50)]\n",
    "    )\n",
    "#Make classifier network\n",
    "gpt2_classifier_network = GPT2ClassifierNetwork.load_from_checkpoint(\"https://f23deeplearning-eredial-classifier-demo.nyc3.digitaloceanspaces.com/gpt2-epoch=59-step=11340-v6.ckpt\", map_location='cpu')\n",
    "\n",
    "#Test code\n",
    "gpt2_trainer.validate(gpt2_classifier_network, gpt2_test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63029314-fdb0-4cfc-b3d7-711b117221bc",
   "metadata": {},
   "source": [
    "### BERT ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ad3d644-3375-4779-81d2-e4545e7e9337",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Downloading: \"https://f23deeplearning-eredial-classifier-demo.nyc3.digitaloceanspaces.com/bert-epoch=59-step=11340-v6.ckpt\" to /home/jovyan/.cache/torch/hub/checkpoints/bert-epoch=59-step=11340-v6.ckpt\n",
      "100%|██████████| 1.23G/1.23G [00:19<00:00, 66.4MB/s]\n",
      "/opt/conda/lib/python3.11/site-packages/lightning/fabric/loggers/csv_logs.py:195: UserWarning: Experiment logs directory lightning_logs/ClassifierTest/bert exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2402cdcb85224d0ca59b01e6c199e271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">  Runningstage.validating  </span>┃<span style=\"font-weight: bold\">                           </span>┃\n",
       "┃<span style=\"font-weight: bold\">          metric           </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          val_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.2199999988079071     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.744948148727417     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m Runningstage.validating \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m                           \u001b[0m┃\n",
       "┃\u001b[1m \u001b[0m\u001b[1m         metric          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m         val_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.2199999988079071    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.744948148727417    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'val_acc': 0.2199999988079071, 'val_loss': 0.744948148727417}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Embed the data so it can be used by the classifiers\n",
    "bert_test_dataset = encoderNetwork(testInput,  testLabels, tokenizer_bert, 'BERT')\n",
    "\n",
    "#Make data loaders\n",
    "bert_test_dataloader = DataLoader(bert_test_dataset, \n",
    "                                  batch_size=TEST_BATCH_SIZE, \n",
    "                                  shuffle=False, collate_fn=custom_collate_fn,\n",
    "                                  num_workers=2)\n",
    "\n",
    "#Make logger\n",
    "bert_logger = pl.loggers.CSVLogger(\"lightning_logs\", name=\"ClassifierTest\", version=\"bert\")\n",
    "\n",
    "\n",
    "#Make trainer\n",
    "bert_trainer = pl.Trainer(\n",
    "        logger=bert_logger,\n",
    "        max_epochs=NUM_EPOCHS,\n",
    "        enable_progress_bar=True,\n",
    "        log_every_n_steps=0,\n",
    "        enable_checkpointing=True,\n",
    "        callbacks=[pl.callbacks.TQDMProgressBar(refresh_rate=50)]\n",
    "    )\n",
    "\n",
    "#Make classifier network\n",
    "bert_classifier_network = BERTClassifierNetwork.load_from_checkpoint(\"https://f23deeplearning-eredial-classifier-demo.nyc3.digitaloceanspaces.com/bert-epoch=59-step=11340-v6.ckpt\", map_location='cpu')\n",
    "\n",
    "#Test code\n",
    "bert_trainer.validate(bert_classifier_network, bert_test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc13da3b-f0d4-4383-b3ff-c5948992d9fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
